{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "06fff33e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import autosklearn.classification\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, accuracy_score, recall_score, precision_score, confusion_matrix, f1_score\n",
    "import matplotlib.pyplot as plt # plotting\n",
    "import numpy as np # linear algebra\n",
    "import os # accessing directory structure\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import dask.dataframe as dd\n",
    "from imblearn.over_sampling import RandomOverSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "22f9912b",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def get_datasets_from_directory(directory_path):\n",
    "    # Check if the given path is a directory\n",
    "    if not os.path.isdir(directory_path):\n",
    "        raise ValueError(\"Provided path is not a directory.\")\n",
    "\n",
    "    # Get a list of all files in the directory with .csv extension\n",
    "    csv_files = [file for file in os.listdir(directory_path) if file.endswith('.csv')]\n",
    "\n",
    "    # Check if there are any CSV files in the directory\n",
    "    if not csv_files:\n",
    "        raise ValueError(\"No CSV files found in the given directory.\")\n",
    "\n",
    "    # Initialize an empty DataFrame to store the concatenated data\n",
    "    concatenated_df = pd.DataFrame()\n",
    "\n",
    "    # Initialize a variable to store the common column names\n",
    "    common_columns = None\n",
    "\n",
    "    # Concatenate each CSV file into the DataFrame\n",
    "    for csv_file in csv_files:\n",
    "        file_path = os.path.join(directory_path, csv_file)\n",
    "        try:\n",
    "            print(file_path)\n",
    "            df = pd.read_csv(file_path, low_memory=False)\n",
    "        except:\n",
    "            print(\"Failed\")\n",
    "            continue\n",
    "\n",
    "        # Check if column names are consistent across CSV files\n",
    "        if common_columns is None:\n",
    "            common_columns = df.columns\n",
    "        else:\n",
    "            if not all(col in df.columns for col in common_columns):\n",
    "                raise ValueError(\"Column names in CSV files are not consistent.\")\n",
    "\n",
    "        concatenated_df = pd.concat([concatenated_df, df], ignore_index=True)\n",
    "\n",
    "    return concatenated_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c61796d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset_from_directories(directories):\n",
    "    if type(directories) == list:\n",
    "        dataframes = []\n",
    "        for directory in directories:\n",
    "            dataframes.append(get_datasets_from_directory(directory))\n",
    "        return pd.concat(dataframes, ignore_index=True)\n",
    "    else:\n",
    "        return get_datasets_from_directory(directories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "828d8709",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def clean_dataset(df):\n",
    "    assert isinstance(df, pd.DataFrame), \"df needs to be a pd.DataFrame\"\n",
    "    df.columns = df.columns.str.strip().str.lower().str.replace(' ', '_').str.replace('(', '').str.replace(')', '')\n",
    "    df.dropna(inplace=True)\n",
    "    df.drop_duplicates(keep=\"first\", inplace=True)\n",
    "    indices_to_keep = ~df.isin([np.nan, np.inf, -np.inf]).any(axis=1)\n",
    "    return df[indices_to_keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "079bef5f",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def standarize_dataset(X):\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    return X_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "259e6376",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def visualize_data(df):\n",
    "    import sweetviz as sv\n",
    "    report_all = sv.analyze(df)\n",
    "    report_all.show_html(filepath=\"SWEETVIZ_result.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be98b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_classification(model, name, X_train, X_test, y_train, y_test):\n",
    "\n",
    "    train_predictions = model.predict(X_train)\n",
    "    test_predictions = model.predict(X_test)\n",
    "\n",
    "\n",
    "    train_accuracy = accuracy_score(y_train, train_predictions)\n",
    "    test_accuracy = accuracy_score(y_test, test_predictions)\n",
    "    \n",
    "    train_precision = precision_score(y_train, train_predictions)\n",
    "    test_precision = precision_score(y_test, test_predictions)\n",
    "    \n",
    "    train_recall = recall_score(y_train, train_predictions)\n",
    "    test_recall = recall_score(y_test, test_predictions)\n",
    "\n",
    "    train_f1 = f1_score(y_train, train_predictions)\n",
    "    test_f1 = f1_score(y_test, test_predictions)\n",
    "\n",
    "    \n",
    "    print(\"Training Accuracy \" + str(name) + \" {}  Test Accuracy \".format(train_accuracy*100) + str(name) + \" {}\".format(test_accuracy*100))\n",
    "    print(\"Training Precesion \" + str(name) + \" {}  Test Precesion \".format(train_precision*100) + str(name) + \" {}\".format(test_precision*100))\n",
    "    print(\"Training Recall \" + str(name) + \" {}  Test Recall \".format(train_recall*100) + str(name) + \" {}\".format(test_recall*100))\n",
    "    print(\"Training F1 \" + str(name) + \" {}  Test F1 \".format(train_f1) + str(name) + \" {}\".format(test_f1))\n",
    "    \n",
    "    actual = y_test\n",
    "    predicted = model.predict(X_test)\n",
    "    confusion_matrix_result = confusion_matrix(actual, predicted)\n",
    "    print(confusion_matrix_result)\n",
    "\n",
    "    # cm_display = ConfusionMatrixDisplay(confusion_matrix = confusion_matrix, display_labels = ['normal', 'attack'])\n",
    "    # fig, ax = plt.subplots(figsize=(10,10))\n",
    "    # ax.grid(False)\n",
    "    # cm_display.plot(ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "da235a76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../datasets/CIC-IDS-2017/MachineLearningCVE/Wednesday-workingHours.pcap_ISCX.csv\n",
      "../../datasets/CIC-IDS-2017/MachineLearningCVE/Tuesday-WorkingHours.pcap_ISCX.csv\n",
      "../../datasets/CIC-IDS-2017/MachineLearningCVE/Thursday-WorkingHours-Morning-WebAttacks.pcap_ISCX.csv\n",
      "../../datasets/CIC-IDS-2017/MachineLearningCVE/Thursday-WorkingHours-Afternoon-Infilteration.pcap_ISCX.csv\n",
      "../../datasets/CIC-IDS-2017/MachineLearningCVE/Monday-WorkingHours.pcap_ISCX.csv\n",
      "../../datasets/CIC-IDS-2017/MachineLearningCVE/Friday-WorkingHours-Morning.pcap_ISCX.csv\n",
      "../../datasets/CIC-IDS-2017/MachineLearningCVE/Friday-WorkingHours-Afternoon-PortScan.pcap_ISCX.csv\n",
      "../../datasets/CIC-IDS-2017/MachineLearningCVE/Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv\n"
     ]
    }
   ],
   "source": [
    "df = get_dataset_from_directories([\"../../datasets/CIC-IDS-2017/MachineLearningCVE/\"])\n",
    "# df = get_dataset_from_directories([\"../datasets/CSE-CIC-IDS2018/\"])\n",
    "# df = get_dataset_from_directories([\"../datasets/CIC-IDS-2017/TrafficLabelling/\", \"../datasets/CIC-IDS-2017/MachineLearningCVE/\"])\n",
    "# df = pd.read_csv(\"../datasets/CIC-IDS-2017/TrafficLabelling/Monday-WorkingHours.pcap_ISCX.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8cd57ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Visualizing the data\n",
    "# visualize_data(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "13d56e49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   destination_port  flow_duration  total_fwd_packets  total_backward_packets  \\\n",
      "0                80          38308                  1                       1   \n",
      "1               389            479                 11                       5   \n",
      "2                88           1095                 10                       6   \n",
      "3               389          15206                 17                      12   \n",
      "4                88           1092                  9                       6   \n",
      "\n",
      "   total_length_of_fwd_packets  total_length_of_bwd_packets  \\\n",
      "0                            6                            6   \n",
      "1                          172                          326   \n",
      "2                         3150                         3150   \n",
      "3                         3452                         6660   \n",
      "4                         3150                         3152   \n",
      "\n",
      "   fwd_packet_length_max  fwd_packet_length_min  fwd_packet_length_mean  \\\n",
      "0                      6                      6                6.000000   \n",
      "1                     79                      0               15.636364   \n",
      "2                   1575                      0              315.000000   \n",
      "3                   1313                      0              203.058823   \n",
      "4                   1575                      0              350.000000   \n",
      "\n",
      "   fwd_packet_length_std  ...  min_seg_size_forward  active_mean  active_std  \\\n",
      "0               0.000000  ...                    20          0.0         0.0   \n",
      "1              31.449238  ...                    32          0.0         0.0   \n",
      "2             632.561635  ...                    32          0.0         0.0   \n",
      "3             425.778474  ...                    32          0.0         0.0   \n",
      "4             694.509719  ...                    32          0.0         0.0   \n",
      "\n",
      "   active_max  active_min  idle_mean  idle_std  idle_max  idle_min   label  \n",
      "0           0           0        0.0       0.0         0         0  BENIGN  \n",
      "1           0           0        0.0       0.0         0         0  BENIGN  \n",
      "2           0           0        0.0       0.0         0         0  BENIGN  \n",
      "3           0           0        0.0       0.0         0         0  BENIGN  \n",
      "4           0           0        0.0       0.0         0         0  BENIGN  \n",
      "\n",
      "[5 rows x 79 columns]\n",
      "Index(['destination_port', 'flow_duration', 'total_fwd_packets',\n",
      "       'total_backward_packets', 'total_length_of_fwd_packets',\n",
      "       'total_length_of_bwd_packets', 'fwd_packet_length_max',\n",
      "       'fwd_packet_length_min', 'fwd_packet_length_mean',\n",
      "       'fwd_packet_length_std', 'bwd_packet_length_max',\n",
      "       'bwd_packet_length_min', 'bwd_packet_length_mean',\n",
      "       'bwd_packet_length_std', 'flow_bytes/s', 'flow_packets/s',\n",
      "       'flow_iat_mean', 'flow_iat_std', 'flow_iat_max', 'flow_iat_min',\n",
      "       'fwd_iat_total', 'fwd_iat_mean', 'fwd_iat_std', 'fwd_iat_max',\n",
      "       'fwd_iat_min', 'bwd_iat_total', 'bwd_iat_mean', 'bwd_iat_std',\n",
      "       'bwd_iat_max', 'bwd_iat_min', 'fwd_psh_flags', 'bwd_psh_flags',\n",
      "       'fwd_urg_flags', 'bwd_urg_flags', 'fwd_header_length',\n",
      "       'bwd_header_length', 'fwd_packets/s', 'bwd_packets/s',\n",
      "       'min_packet_length', 'max_packet_length', 'packet_length_mean',\n",
      "       'packet_length_std', 'packet_length_variance', 'fin_flag_count',\n",
      "       'syn_flag_count', 'rst_flag_count', 'psh_flag_count', 'ack_flag_count',\n",
      "       'urg_flag_count', 'cwe_flag_count', 'ece_flag_count', 'down/up_ratio',\n",
      "       'average_packet_size', 'avg_fwd_segment_size', 'avg_bwd_segment_size',\n",
      "       'fwd_header_length.1', 'fwd_avg_bytes/bulk', 'fwd_avg_packets/bulk',\n",
      "       'fwd_avg_bulk_rate', 'bwd_avg_bytes/bulk', 'bwd_avg_packets/bulk',\n",
      "       'bwd_avg_bulk_rate', 'subflow_fwd_packets', 'subflow_fwd_bytes',\n",
      "       'subflow_bwd_packets', 'subflow_bwd_bytes', 'init_win_bytes_forward',\n",
      "       'init_win_bytes_backward', 'act_data_pkt_fwd', 'min_seg_size_forward',\n",
      "       'active_mean', 'active_std', 'active_max', 'active_min', 'idle_mean',\n",
      "       'idle_std', 'idle_max', 'idle_min', 'label'],\n",
      "      dtype='object') 2520798 79\n",
      "['BENIGN' 'DoS slowloris' 'DoS Slowhttptest' 'DoS Hulk' 'DoS GoldenEye'\n",
      " 'Heartbleed' 'FTP-Patator' 'SSH-Patator' 'Web Attack � Brute Force'\n",
      " 'Web Attack � XSS' 'Web Attack � Sql Injection' 'Infiltration' 'Bot'\n",
      " 'PortScan' 'DDoS']\n",
      "label\n",
      "BENIGN                        2095057\n",
      "DoS Hulk                       172846\n",
      "DDoS                           128014\n",
      "PortScan                        90694\n",
      "DoS GoldenEye                   10286\n",
      "FTP-Patator                      5931\n",
      "DoS slowloris                    5385\n",
      "DoS Slowhttptest                 5228\n",
      "SSH-Patator                      3219\n",
      "Bot                              1948\n",
      "Web Attack � Brute Force         1470\n",
      "Web Attack � XSS                  652\n",
      "Infiltration                       36\n",
      "Web Attack � Sql Injection         21\n",
      "Heartbleed                         11\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df = clean_dataset(df)\n",
    "\n",
    "print(df.head())\n",
    "print(df.columns, len(df), len(df.columns))\n",
    "print(df[\"label\"].unique())\n",
    "print(df[\"label\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a8720062",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "0.0    2095057\n",
      "1.0     425741\n",
      "Name: count, dtype: int64\n",
      "   destination_port  flow_duration  total_fwd_packets  total_backward_packets  \\\n",
      "0                80          38308                  1                       1   \n",
      "1               389            479                 11                       5   \n",
      "2                88           1095                 10                       6   \n",
      "3               389          15206                 17                      12   \n",
      "4                88           1092                  9                       6   \n",
      "\n",
      "   total_length_of_fwd_packets  total_length_of_bwd_packets  \\\n",
      "0                            6                            6   \n",
      "1                          172                          326   \n",
      "2                         3150                         3150   \n",
      "3                         3452                         6660   \n",
      "4                         3150                         3152   \n",
      "\n",
      "   fwd_packet_length_max  fwd_packet_length_min  fwd_packet_length_mean  \\\n",
      "0                      6                      6                6.000000   \n",
      "1                     79                      0               15.636364   \n",
      "2                   1575                      0              315.000000   \n",
      "3                   1313                      0              203.058823   \n",
      "4                   1575                      0              350.000000   \n",
      "\n",
      "   fwd_packet_length_std  ...  min_seg_size_forward  active_mean  active_std  \\\n",
      "0               0.000000  ...                    20          0.0         0.0   \n",
      "1              31.449238  ...                    32          0.0         0.0   \n",
      "2             632.561635  ...                    32          0.0         0.0   \n",
      "3             425.778474  ...                    32          0.0         0.0   \n",
      "4             694.509719  ...                    32          0.0         0.0   \n",
      "\n",
      "   active_max  active_min  idle_mean  idle_std  idle_max  idle_min  label  \n",
      "0           0           0        0.0       0.0         0         0    0.0  \n",
      "1           0           0        0.0       0.0         0         0    0.0  \n",
      "2           0           0        0.0       0.0         0         0    0.0  \n",
      "3           0           0        0.0       0.0         0         0    0.0  \n",
      "4           0           0        0.0       0.0         0         0    0.0  \n",
      "\n",
      "[5 rows x 79 columns]\n"
     ]
    }
   ],
   "source": [
    "# One Hot Encode the lable column and then only take values that are Benign\n",
    "onehotencoder = OneHotEncoder()\n",
    "labels = df[\"label\"].values.reshape(-1, 1)\n",
    "labels = onehotencoder.fit_transform(labels).toarray()\n",
    "encoded_labels = np.logical_not(labels[:,0])\n",
    "\n",
    "df[\"label\"] = encoded_labels\n",
    "df[\"label\"] = df[\"label\"].astype(float)\n",
    "print(df[\"label\"].value_counts())\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "98af207b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split X and y\n",
    "X, y = df.drop(\"label\", axis=1).to_numpy(), df[[\"label\"]].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b2b32050",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale Data\n",
    "X = standarize_dataset(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e2eab874",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of original features is 78 and of reduced features is 30\n"
     ]
    }
   ],
   "source": [
    "# PCA Feature and Dimentionality Reduction\n",
    "n_components = 30\n",
    "pca = PCA(n_components=n_components)\n",
    "pca = pca.fit(X)\n",
    "X_reduced = pca.transform(X)\n",
    "print(\"Number of original features is {} and of reduced features is {}\".format(X.shape[1], X_reduced.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbceadda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into training and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, shuffle=True, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e47b389",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Oversampling\n",
    "# sampler = RandomOverSampler(sampling_strategy=\"all\")\n",
    "# X_train, y_train = sampler.fit_resample(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a1240a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
